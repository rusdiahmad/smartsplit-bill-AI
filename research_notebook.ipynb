## research_notebook.ipynb

# Import Libraries
import time
from PIL import Image
from transformers import DonutProcessor, VisionEncoderDecoderModel
import torch
import json

# --- Konfigurasi Model Donut (Model 1) ---
# Menggunakan model Donut yang dilatih untuk Receipt Parsing (CORD dataset)
model_name = "naver-clova-ix/donut-base-finetuned-cord-v2"
processor = DonutProcessor.from_pretrained(model_name)
model = VisionEncoderDecoderModel.from_pretrained(model_name)

# Pindahkan model ke GPU jika tersedia
device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

def run_inference(image_path, prompt, model, processor):
    """Fungsi untuk menjalankan inference dan mengukur latensi."""
    image = Image.open(image_path).convert("RGB")

    # Siapkan input
    pixel_values = processor(image, return_tensors="pt").pixel_values
    decoder_input_ids = processor.tokenizer(prompt, add_special_tokens=False, return_tensors="pt").input_ids

    start_time = time.time()
    
    # Jalankan inference
    output_ids = model.generate(
        pixel_values.to(device),
        decoder_input_ids=decoder_input_ids.to(device),
        max_length=model.decoder.config.max_length,
        early_stopping=True,
        pad_token_id=processor.tokenizer.pad_token_id,
        eos_token_id=processor.tokenizer.eos_token_id,
        use_cache=True,
        num_beams=1,
        bad_words_ids=[[processor.tokenizer.unk_token_id]],
        return_dict_in_generate=True,
    ).sequences
    
    end_time = time.time()
    latency = end_time - start_time
    
    # Decode output
    output = processor.batch_decode(output_ids[:, decoder_input_ids.shape[1]:])[0]
    output = processor.token2json(output)
    
    return output, latency

# --- Eksperimen dengan Nota 1 (contoh1.jpg) ---
image_path_1 = "images/contoh1.jpg"
prompt_1 = "<s>" # Prompt default untuk Donut
print(f"--- Running Donut on {image_path_1} ---")

output_1, latency_1 = run_inference(image_path_1, prompt_1, model, processor)
print(f"Latency: {latency_1:.4f} seconds")
# Format output agar mudah dibaca
print("\nExtracted JSON:")
print(json.dumps(output_1, indent=2))

# --- Eksperimen dengan Nota 2 (contoh2.jpg) ---
image_path_2 = "images/contoh2.jpg"
prompt_2 = "<s>" 
print(f"\n--- Running Donut on {image_path_2} ---")

output_2, latency_2 = run_inference(image_path_2, prompt_2, model, processor)
print(f"Latency: {latency_2:.4f} seconds")
print("\nExtracted JSON:")
print(json.dumps(output_2, indent=2))

# Lanjutkan dengan kode untuk Model 2 dan buat Tabel Komparasi.
# Simpan output dari notebook ini.
